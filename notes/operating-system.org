* threads
  - thread control block
    + thread characteristics: thread id, name of program
    + state informaation: instruction counter, stack pointer, register contents
    + management data: priority, rights, statistics

      [[./images/opearting-system/process-control-block.png]]
  - static operating system: all threads are known in advance and statically defined
  - dynamic operating system: the threads are createed and delted by kernel operations

  - threads and address spaces
    + an arbitrary number of /logical address spaces/ that can be mapped to the physical address space automatically
    + mutual protection of address spaces
    + address spaces are independent of threads
    + a thread owns exactly one private address space( unix process)
    + several threads share an address space (threads)
    + a thread switches from one address space to another
  - Thread switch: means that the processor stops the execution of the current thread and continues with the execution of another thread
    + by jumping: apply in real-time system, but inflexible and applicable only in very special cases
    + In general the thread switch will be costly
      - we don't know , from where we return to the interrupted thread (memorizing the continuation address)
        before switching to the new thread, we store the address of next instruction to be executed in a dedicated variable ~ni~ (next instruction) of thread control block
      - the next thread, to which we switch, is not always the same (selection of next thread)
        #+BEGIN_EXAMPLE
          t_next := TSelect()              // select next thread to run
          t_run.ni := L                    // store address L in variable ni
          jmp (t_next.ni)
        L:
        #+END_EXAMPLE
      - the processor contains essential part of the thread description that must not get lost( register reload)
    + criteria for selection: number of thread (cyclic switching), order of arrival, priority (urgency)
    + processor registers
      - threads use arithmetic registers of the processor to store intermediate results, simply jump to next thread, their content will be lost
    + thread context:
      - instruction pointer
      - contents of arithmetic registers, index registers, processor state, which represent the *state of the execution* of the program
      - contents of address registers, segment tables, interrupt masks, access control information, thread's execution environment
    + context switch: the most time consuming part of thread switch
      - by special instructions that allow storing the complete set of registers to the memory in one instruction
      - by providing several sets of registers on the processor, such that at thread switch only the register need to be saved that indicates the
        number of the currently valid register set.

      - Thread switch can be comparably fast, if only the arithmetic registers need to saved and reloaded while the addressing environment remain constant (lightweight threads)

  #+BEGIN_EXAMPLE
  switch -> save context of t_run -> select t_next -> save next instruction of t_run -> jump to t_next.ni -> t_run := t_next -> load context of t_run
  #+END_EXAMPLE

  - thread states: running/ready/waiting/suspend(swaping)/creation/termination
    + we used the *conditioned switch* to release the processor, when the current thread could not continue execution for some reason
    + in this case we switch to another thread. This new thread, however, may also be blocked, e.g. because it waits for the completion of an I/O operation.
      If we switch to such a thread, the processor would be immediately released again.
    + This way, we could try one thread after another until we finally may detect a thread that is ready to resume execution.
    + to speed up the search for a ready thread, we combine threads according to their state(resumable, not resumable) to thread subsets

  - state change operations
    + relinquish: voluntary switching to another thread.
    + assign: take the next thread from the ready set to resume its operation on the processor
    + deblock: if the event happened for which the blocked thread waited it changes its state from waiting to ready and is inserted into the set of ready threads
    + block: leave the processor since some condition does not hold (conditioned switch).

  - when execution the state transitions we have to distinguish:
    + the state change operation themselves
    + other actions that may be neccessasry related to the state change
  - the state chagne as an operation at the TCP data structure.
    [[./images/operating-system/thread-state-transition.png]]

    [[./images/operating-system/thread-states-in-unix.png]]

  - a thread reamins being executed until:
    + it voluntarily gives up the execution (relinquish)
    + it is forced to give up execution by a clock interrupt
    + it cannot continue due to some condition it is waiting for
  - the current thread is being *preempted* by the thread with the higher priority

  - Idle problem
    + when operating with waiting states it may happen that all threads are blocked since they are waiting for something. In this case the processor has nothing to do
    + to handle this situation in an elegant and consistent way we simply introduce an *idle thread*.
      - must not stop (cyclic thread, endless loop)
      - lowest priority(to be preempted by any real thread)
      - must be preemptable at any time
      - examples:
        + loop while true; wastes energy
        + dynamic stop
        + insertion of useful housekeeping tasks: checks, reorganizations, garbage collections.

  - Initialization  problem
    + the thread starts and ends in the "kernel of the kernel", in the procedure "switch"

  - Kernel operations for thread management !
    + CREATE_THREAD
    + DELETE_THREAD
    + SET_ATTRIBUTE
    + READ_ATTRIBUTE
    + RELINQUISH_THREAD
    + BLOCK_THREAD
    + DEBLOCK_THREAD
    + ACTIVATE_THREAD
    + DEACTIVATE_THREAD
  - relation of programming language threads to OS threads
    +
* virtual memory
  - address translation
    + Logical address space, program address space
    + Physical address space (defined by the width of the address bus)
  - Two-stage hierarchical addresss translation
    + table base address
    + program/data address: segment/page/byte
    + segment table
    + page table
    + memory
  - example:
    + 32 bit address
    + 4 GB logical address space
    + 64 MB RAM (physical)
    + Pages of 1 KB
    + One page table for the whole logical address space
    + page table
      - address (32 bits) = page address (22 bits) + offset in page (10 bits)
      - offset (inside pages): 10 bit (2^10 = 1 KB)
      - Page address: 22 bits ( 32 - 10)
      - number of entries in page table: 2^22 = 4M
      - size of an entry: 16 bits = 2 bytes, 64 MB = 2^26 B = 2^16 frames
      - size of page table (ignoring managament information such as dirty bits etc. and ignoring alignment): 8 MB ( 4 M * 2 bytes)
    + inverted page table
      - offset (inside pages): 10 bits
      - number of frames: 65536 = 2 ^ 16
      - frame address: 16 bit
      - number of entries in inverted page table: 2^16 = 64K
      - size of an entry: 22 bit = 2.75 bytes, page addresses are 22 bit
      - size of page table: 176KB = 64K * 2.75 Bytes

  - acceleration of address translation
    + problems:
      - segment and page tables are so large that they have to be kept in main memory.
      - to build an effective main memory address, we first need to get the page and/or segment address
      - thus, the processing speed is reduced by a factor of 2
    + solution:
      - to prevent that, the currently used parts of the segment/page tables are stored in a fast set of registers (TLB = Translation Lookaside Buffer, part of MMU)
      - The TLB is an completely associative memory, i.e. a table in which the entry to be found is being searched simultaneously in all lines of the table
      - it's used as a sort of cache for page/segment tables
      - usually, the search can be performed in one processor cycle.

    + properities of TLB
      - line width: 4-8 bytes: logical page/segment number, page frame number, management bits
      - time for address translation:
        + hit: less than 1 processor cycle
        + miss: 10 - 200 processor cycles (depending on memory speed)
      - hit rate: 99.0% - 99.99%
      - tlb-size: 32 - 1024 lines(entries)
      - why not larger TLB?
      - what happens when a thread switch occurs
    + memory protection for hierarchical address translation
  - locality
    + spatial locality: when a program accesses an address /a/, then another access to a nearby address is very likely
    + temporal locality: when a program accesses an address /a/, then a repeated access to the same address within short time is very likely
    + why ?
      - mostly, instructions are executed sequentially.
      - programs spend the most time in loops
      - some parts of the program are executed only in exceptional cases
      - many arrays are only partially filled.
      - 90/10 rules: a thread spends 90% of its time in 10 ^ of its address space.

  - virtual memory
    + the pages needed are loaded only when addressed(demand paging)
    + requirements for efficient operation
    + noncontiguous allocation ( page tables ), pages are the units of transfer
    + automatic detection of missing pages
      - access to missing page triggers interrupt.
      - loading of page from disk is initiated as part of the interrupt handling.

    + components:
      - page table
        + function: address transformationo
        + content: for each page:
          - usage and presence information
          - physical address (page frame number)
        + presences bit/valid bit, reference bit, modification bit/dirty bit
      - page frame table, inverted page table
        + function: memory management
        + content: for each page frame
          - state(free/occupied)
          - owner
          - occupying page
      - swap area (paging area)
        + function: areas of storage to store the pages that are swapped out
          - usually mass storage such as magnetic or solid state disks
          - seldom network devices

  - page fault
  - parallelization of paing
    + paging is a time-critical component, we therefore try to speed it up by parallelizatioin
    + buffering:
      - since page faults often occur in bulks, it is recommend to have some amount of free page
        frames available to avoid costly page-out operations when time is tight.
      - to that purpose we parallelize by applying the buffering principle to get a stock of free page
  - page replacement strategies
    + it's of utmost importance to keep the number of page faults extremely low
  - selection strategy: when a page fault occurs and no page frame is free, which page frame should be emptied?
    + local selection strategy: we clear a page frame of *that process* that caused the page fault
    + global selection strategy: an arbitrary page frame (maybe belonging to *other processes*) is cleared.
    + FIFO/LFU/LRU/RNU/Clock-Algorithm/
  - locality is good, if few pages are referenced with high probability, and many pages with low probability
  - Thrashing effect: the system is completely occupied with paging and cannot perform regular useful work.
    + goal: high processor utilization
    + many programs executed simultaneously
    + high multiprogramming degree n
    + low memory space s per process
    + short time between successive page faults
    + congestion at paging device(disk)
    + almost all processes blocked
    + result: poor processor utilization
  - overload phenomena
    + computer networks: too many packets
    + telephone networks, too many calls
    + database system, too many transactions
    + parallel computing, too many processors
    + reason is, overhead for coordination grows overlinearly
  - trashing prevention
  - local control of paging activity
    + the working-set model
    + page fault frequency model (PFF)
  - global control of pagin activity
    + the criterion of the interpagefault time (L = S-criterion)
    + the time between two page faults t_s (or L resp.) should be roughly the same as the page transfer time t_{T} (or S, resp)
    + the resulting operation point is in most cases too far at the right which can be taken into account in the control laws
    + parabola approximation: the thrashing curve can be approximated by a parabola in the region of the maximum.
